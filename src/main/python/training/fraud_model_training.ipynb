{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-02T17:54:58.242946Z",
     "start_time": "2025-05-02T17:54:58.206171Z"
    }
   },
   "cell_type": "code",
   "source": [
    "{\n",
    "    \"cells\": [\n",
    "        {\n",
    "            \"cell_type\": \"markdown\",\n",
    "            \"metadata\": {},\n",
    "            \"source\": [\n",
    "                \"# MoMoGuard-GH: Fraud Detection Model Training\\n\",\n",
    "                \"\\n\",\n",
    "                \"This notebook demonstrates the process of training a machine learning model to detect fraudulent mobile money transactions. For the MVP, we'll use synthetic data that simulates typical fraud patterns in Ghana's mobile money ecosystem.\"\n",
    "            ]\n",
    "        },\n",
    "        {\n",
    "            \"cell_type\": \"code\",\n",
    "            \"execution_count\": 1,\n",
    "            \"metadata\": {},\n",
    "            \"source\": [\n",
    "                \"import numpy as np\\n\",\n",
    "                \"import pandas as pd\\n\",\n",
    "                \"import matplotlib.pyplot as plt\\n\",\n",
    "                \"import seaborn as sns\\n\",\n",
    "                \"from sklearn.model_selection import train_test_split, GridSearchCV\\n\",\n",
    "                \"from sklearn.ensemble import RandomForestClassifier\\n\",\n",
    "                \"from sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc\\n\",\n",
    "                \"from imblearn.over_sampling import SMOTE\\n\",\n",
    "                \"import joblib\\n\",\n",
    "                \"\\n\",\n",
    "                \"# Set random seed for reproducibility\\n\",\n",
    "                \"np.random.seed(42)\"\n",
    "            ]\n",
    "        },\n",
    "        {\n",
    "            \"cell_type\": \"markdown\",\n",
    "            \"metadata\": {},\n",
    "            \"source\": [\n",
    "                \"## 1. Generate Synthetic Mobile Money Transaction Data\\n\",\n",
    "                \"\\n\",\n",
    "                \"Since real mobile money transaction data is sensitive and not easily available, we'll create synthetic data that resembles typical mobile money transactions in Ghana, with some fraudulent patterns included.\"\n",
    "            ]\n",
    "        },\n",
    "        {\n",
    "            \"cell_type\": \"code\",\n",
    "            \"execution_count\": 2,\n",
    "            \"metadata\": {},\n",
    "            \"source\": [\n",
    "                \"def generate_synthetic_data(n_samples=10000, fraud_ratio=0.05):\\n\",\n",
    "                \"    \\\"\\\"\\\"\\n\",\n",
    "                \"    Generate synthetic mobile money transaction data with fraud indicators\\n\",\n",
    "                \"    \\n\",\n",
    "                \"    Parameters:\\n\",\n",
    "                \"    n_samples (int): Number of transactions to generate\\n\",\n",
    "                \"    fraud_ratio (float): Ratio of fraudulent transactions\\n\",\n",
    "                \"    \\n\",\n",
    "                \"    Returns:\\n\",\n",
    "                \"    DataFrame: Synthetic transaction data\\n\",\n",
    "                \"    \\\"\\\"\\\"\\n\",\n",
    "                \"    # Number of fraudulent and legitimate transactions\\n\",\n",
    "                \"    n_fraud = int(n_samples * fraud_ratio)\\n\",\n",
    "                \"    n_legitimate = n_samples - n_fraud\\n\",\n",
    "                \"    \\n\",\n",
    "                \"    # Generate legitimate transactions\\n\",\n",
    "                \"    legitimate_data = {\\n\",\n",
    "                \"        'amount': np.random.gamma(2, 100, n_legitimate),  # Typical transaction amounts\\n\",\n",
    "                \"        'is_foreign_receiver': np.random.choice([0, 1], size=n_legitimate, p=[0.95, 0.05]),  # Most receivers are local\\n\",\n",
    "                \"        'num_recent_transactions': np.random.poisson(10, n_legitimate),  # Average transaction history\\n\",\n",
    "                \"        'avg_transaction_amount': np.random.gamma(2, 80, n_legitimate),  # Typical average amounts\\n\",\n",
    "                \"        'transaction_frequency_change': np.random.normal(0, 0.3, n_legitimate),  # Stable transaction patterns\\n\",\n",
    "                \"        'is_new_receiver': np.random.choice([0, 1], size=n_legitimate, p=[0.7, 0.3]),  # Most receivers are known\\n\",\n",
    "                \"        'time_of_day_risk': np.random.choice([0, 1], size=n_legitimate, p=[0.85, 0.15]),  # Mostly daytime transactions\\n\",\n",
    "                \"        'is_fraud': np.zeros(n_legitimate)  # Not fraud\\n\",\n",
    "                \"    }\\n\",\n",
    "                \"    \\n\",\n",
    "                \"    # Generate fraudulent transactions with distinct patterns\\n\",\n",
    "                \"    fraud_data = {\\n\",\n",
    "                \"        'amount': np.random.gamma(5, 150, n_fraud),  # Higher amounts on average\\n\",\n",
    "                \"        'is_foreign_receiver': np.random.choice([0, 1], size=n_fraud, p=[0.6, 0.4]),  # More foreign receivers\\n\",\n",
    "                \"        'num_recent_transactions': np.random.poisson(3, n_fraud),  # Less transaction history\\n\",\n",
    "                \"        'avg_transaction_amount': np.random.gamma(1.5, 50, n_fraud),  # Lower historical amounts\\n\",\n",
    "                \"        'transaction_frequency_change': np.random.normal(1.5, 0.8, n_fraud),  # Sudden increase in frequency\\n\",\n",
    "                \"        'is_new_receiver': np.random.choice([0, 1], size=n_fraud, p=[0.2, 0.8]),  # Mostly new receivers\\n\",\n",
    "                \"        'time_of_day_risk': np.random.choice([0, 1], size=n_fraud, p=[0.3, 0.7]),  # More night transactions\\n\",\n",
    "                \"        'is_fraud': np.ones(n_fraud)  # Is fraud\\n\",\n",
    "                \"    }\\n\",\n",
    "                \"    \\n\",\n",
    "                \"    # Combine legitimate and fraudulent data\\n\",\n",
    "                \"    for key in legitimate_data:\\n\",\n",
    "                \"        legitimate_data[key] = np.concatenate([legitimate_data[key], fraud_data[key]])\\n\",\n",
    "                \"        \\n\",\n",
    "                \"    # Create DataFrame\\n\",\n",
    "                \"    df = pd.DataFrame(legitimate_data)\\n\",\n",
    "                \"    \\n\",\n",
    "                \"    # Shuffle the data\\n\",\n",
    "                \"    df = df.sample(frac=1).reset_index(drop=True)\\n\",\n",
    "                \"    \\n\",\n",
    "                \"    return df\\n\",\n",
    "                \"\\n\",\n",
    "                \"# Generate data\\n\",\n",
    "                \"transactions_df = generate_synthetic_data()\\n\",\n",
    "                \"\\n\",\n",
    "                \"# Display first few rows\\n\",\n",
    "                \"transactions_df.head()\"\n",
    "            ]\n",
    "        },\n",
    "        {\n",
    "            \"cell_type\": \"code\",\n",
    "            \"execution_count\": 3,\n",
    "            \"metadata\": {},\n",
    "            \"source\": [\n",
    "                \"# Save synthetic data to CSV\\n\",\n",
    "                \"transactions_df.to_csv('../data/sim_swap_fraud.csv', index=False)\\n\",\n",
    "                \"\\n\",\n",
    "                \"# Basic statistics\\n\",\n",
    "                \"print(f\\\"Total transactions: {len(transactions_df)}\\\")\\n\",\n",
    "                \"print(f\\\"Fraudulent transactions: {transactions_df['is_fraud'].sum()} ({transactions_df['is_fraud'].mean()*100:.2f}%)\\\")\"\n",
    "            ]\n",
    "        },\n",
    "        {\n",
    "            \"cell_type\": \"markdown\",\n",
    "            \"metadata\": {},\n",
    "            \"source\": [\n",
    "                \"## 2. Exploratory Data Analysis\"\n",
    "            ]\n",
    "        },\n",
    "        {\n",
    "            \"cell_type\": \"code\",\n",
    "            \"execution_count\": 4,\n",
    "            \"metadata\": {},\n",
    "            \"source\": [\n",
    "                \"# Summary statistics\\n\",\n",
    "                \"print(\\\"Summary statistics for all transactions:\\\")\\n\",\n",
    "                \"transactions_df.describe()\\n\",\n",
    "                \"\\n\",\n",
    "                \"# Summary statistics by fraud status\\n\",\n",
    "                \"print(\\\"\\\\nSummary statistics for legitimate transactions:\\\")\\n\",\n",
    "                \"transactions_df[transactions_df['is_fraud'] == 0].describe()\\n\",\n",
    "                \"\\n\",\n",
    "                \"print(\\\"\\\\nSummary statistics for fraudulent transactions:\\\")\\n\",\n",
    "                \"transactions_df[transactions_df['is_fraud'] == 1].describe()\"\n",
    "            ]\n",
    "        },\n",
    "        {\n",
    "            \"cell_type\": \"code\",\n",
    "            \"execution_count\": 5,\n",
    "            \"metadata\": {},\n",
    "            \"source\": [\n",
    "                \"# Set up the visualization style\\n\",\n",
    "                \"plt.style.use('seaborn-whitegrid')\\n\",\n",
    "                \"plt.rcParams['figure.figsize'] = (12, 8)\\n\",\n",
    "                \"plt.rcParams['font.size'] = 12\\n\",\n",
    "                \"\\n\",\n",
    "                \"# Distribution of transaction amounts by fraud status\\n\",\n",
    "                \"plt.figure(figsize=(12, 6))\\n\",\n",
    "                \"sns.histplot(data=transactions_df, x='amount', hue='is_fraud', bins=50, kde=True, element='step')\\n\",\n",
    "                \"plt.title('Distribution of Transaction Amounts by Fraud Status')\\n\",\n",
    "                \"plt.xlabel('Transaction Amount (GHS)')\\n\",\n",
    "                \"plt.ylabel('Count')\\n\",\n",
    "                \"plt.legend(['Legitimate', 'Fraudulent'])\\n\",\n",
    "                \"plt.xlim(0, 1500)  # Limit x-axis for better visualization\\n\",\n",
    "                \"plt.show()\\n\",\n",
    "                \"\\n\",\n",
    "                \"# Transaction frequency change by fraud status\\n\",\n",
    "                \"plt.figure(figsize=(12, 6))\\n\",\n",
    "                \"sns.boxplot(data=transactions_df, x='is_fraud', y='transaction_frequency_change')\\n\",\n",
    "                \"plt.title('Transaction Frequency Change by Fraud Status')\\n\",\n",
    "                \"plt.xlabel('Is Fraud')\\n\",\n",
    "                \"plt.ylabel('Transaction Frequency Change')\\n\",\n",
    "                \"plt.xticks([0, 1], ['Legitimate', 'Fraudulent'])\\n\",\n",
    "                \"plt.show()\"\n",
    "            ]\n",
    "        },\n",
    "        {\n",
    "            \"cell_type\": \"code\",\n",
    "            \"execution_count\": 6,\n",
    "            \"metadata\": {},\n",
    "            \"source\": [\n",
    "                \"# Correlation matrix\\n\",\n",
    "                \"plt.figure(figsize=(10, 8))\\n\",\n",
    "                \"correlation_matrix = transactions_df.corr()\\n\",\n",
    "                \"sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', vmin=-1, vmax=1, fmt='.2f')\\n\",\n",
    "                \"plt.title('Correlation Matrix of Transaction Features')\\n\",\n",
    "                \"plt.show()\\n\",\n",
    "                \"\\n\",\n",
    "                \"# Feature distribution comparisons\\n\",\n",
    "                \"fig, axes = plt.subplots(2, 3, figsize=(18, 10))\\n\",\n",
    "                \"axes = axes.flatten()\\n\",\n",
    "                \"\\n\",\n",
    "                \"# Plot histograms for numerical features\\n\",\n",
    "                \"numerical_features = ['amount', 'num_recent_transactions', 'avg_transaction_amount', \\n\",\n",
    "                \"                     'transaction_frequency_change']\\n\",\n",
    "                \"\\n\",\n",
    "                \"for i, feature in enumerate(numerical_features):\\n\",\n",
    "                \"    sns.histplot(data=transactions_df, x=feature, hue='is_fraud', kde=True, element='step', ax=axes[i])\\n\",\n",
    "                \"    axes[i].set_title(f'Distribution of {feature.replace(\\\"_\\\", \\\" \\\").title()}')\\n\",\n",
    "                \"    axes[i].legend(['Legitimate', 'Fraudulent'])\\n\",\n",
    "                \"\\n\",\n",
    "                \"# Plot count plots for categorical features\\n\",\n",
    "                \"categorical_features = ['is_foreign_receiver', 'is_new_receiver', 'time_of_day_risk']\\n\",\n",
    "                \"for i, feature in enumerate(categorical_features):\\n\",\n",
    "                \"    sns.countplot(data=transactions_df, x=feature, hue='is_fraud', ax=axes[i+len(numerical_features)])\\n\",\n",
    "                \"    axes[i+len(numerical_features)].set_title(f'Count of {feature.replace(\\\"_\\\", \\\" \\\").title()}')\\n\",\n",
    "                \"    axes[i+len(numerical_features)].legend(['Legitimate', 'Fraudulent'])\\n\",\n",
    "                \"\\n\",\n",
    "                \"plt.tight_layout()\\n\",\n",
    "                \"plt.show()\"\n",
    "            ]\n",
    "        },\n",
    "        {\n",
    "            \"cell_type\": \"markdown\",\n",
    "            \"metadata\": {},\n",
    "            \"source\": [\n",
    "                \"## 3. Feature Engineering\\n\",\n",
    "                \"\\n\",\n",
    "                \"Let's create some additional features that might be helpful for fraud detection.\"\n",
    "            ]\n",
    "        },\n",
    "        {\n",
    "            \"cell_type\": \"code\",\n",
    "            \"execution_count\": 7,\n",
    "            \"metadata\": {},\n",
    "            \"source\": [\n",
    "                \"def engineer_features(df):\\n\",\n",
    "                \"    \\\"\\\"\\\"\\n\",\n",
    "                \"    Create new features for fraud detection\\n\",\n",
    "                \"    \\n\",\n",
    "                \"    Parameters:\\n\",\n",
    "                \"    df (DataFrame): Original transaction data\\n\",\n",
    "                \"    \\n\",\n",
    "                \"    Returns:\\n\",\n",
    "                \"    DataFrame: Data with additional features\\n\",\n",
    "                \"    \\\"\\\"\\\"\\n\",\n",
    "                \"    # Create a copy to avoid modifying the original\\n\",\n",
    "                \"    df_new = df.copy()\\n\",\n",
    "                \"    \\n\",\n",
    "                \"    # Amount ratio compared to average (unusual amounts)\\n\",\n",
    "                \"    df_new['amount_avg_ratio'] = df_new['amount'] / (df_new['avg_transaction_amount'] + 1)  # +1 to avoid division by zero\\n\",\n",
    "                \"    \\n\",\n",
    "                \"    # Combine risk factors\\n\",\n",
    "                \"    df_new['combined_risk_score'] = (\\n\",\n",
    "                \"        df_new['is_foreign_receiver'] * 2 +\\n\",\n",
    "                \"        df_new['is_new_receiver'] * 1.5 +\\n\",\n",
    "                \"        df_new['time_of_day_risk'] * 1 +\\n\",\n",
    "                \"        (df_new['transaction_frequency_change'] > 1) * 2\\n\",\n",
    "                \"    )\\n\",\n",
    "                \"    \\n\",\n",
    "                \"    # Transaction amount risk (high amounts are riskier)\\n\",\n",
    "                \"    df_new['amount_risk'] = np.log1p(df_new['amount']) / 10  # Log-transform and scale\\n\",\n",
    "                \"    \\n\",\n",
    "                \"    # Transaction history risk (fewer transactions means higher risk)\\n\",\n",
    "                \"    df_new['history_risk'] = np.exp(-df_new['num_recent_transactions'] / 10)\\n\",\n",
    "                \"    \\n\",\n",
    "                \"    # Compute overall risk score\\n\",\n",
    "                \"    df_new['risk_score'] = (\\n\",\n",
    "                \"        df_new['combined_risk_score'] * 0.4 +\\n\",\n",
    "                \"        df_new['amount_risk'] * 0.3 +\\n\",\n",
    "                \"        df_new['history_risk'] * 0.3\\n\",\n",
    "                \"    )\\n\",\n",
    "                \"    \\n\",\n",
    "                \"    return df_new\\n\",\n",
    "                \"\\n\",\n",
    "                \"# Apply feature engineering\\n\",\n",
    "                \"enhanced_df = engineer_features(transactions_df)\\n\",\n",
    "                \"\\n\",\n",
    "                \"# Display first few rows with new features\\n\",\n",
    "                \"enhanced_df.head()\"\n",
    "            ]\n",
    "        },\n",
    "        {\n",
    "            \"cell_type\": \"code\",\n",
    "            \"execution_count\": 8,\n",
    "            \"metadata\": {},\n",
    "            \"source\": [\n",
    "                \"# Analyze new features\\n\",\n",
    "                \"plt.figure(figsize=(12, 6))\\n\",\n",
    "                \"sns.boxplot(data=enhanced_df, x='is_fraud', y='risk_score')\\n\",\n",
    "                \"plt.title('Risk Score by Fraud Status')\\n\",\n",
    "                \"plt.xlabel('Is Fraud')\\n\",\n",
    "                \"plt.ylabel('Risk Score')\\n\",\n",
    "                \"plt.xticks([0, 1], ['Legitimate', 'Fraudulent'])\\n\",\n",
    "                \"plt.show()\\n\",\n",
    "                \"\\n\",\n",
    "                \"# Correlation of new features with fraud\\n\",\n",
    "                \"new_features_corr = enhanced_df[['amount_avg_ratio', 'combined_risk_score', 'amount_risk', \\n\",\n",
    "                \"                               'history_risk', 'risk_score', 'is_fraud']].corr()['is_fraud'].sort_values()\\n\",\n",
    "                \"plt.figure(figsize=(10, 6))\\n\",\n",
    "                \"new_features_corr.drop('is_fraud').plot(kind='barh')\\n\",\n",
    "                \"plt.title('Correlation of New Features with Fraud')\\n\",\n",
    "                \"plt.xlabel('Correlation Coefficient')\\n\",\n",
    "                \"plt.tight_layout()\\n\",\n",
    "                \"plt.show()\"\n",
    "            ]\n",
    "        },\n",
    "        {\n",
    "            \"cell_type\": \"markdown\",\n",
    "            \"metadata\": {},\n",
    "            \"source\": [\n",
    "                \"## 4. Model Training\\n\",\n",
    "                \"\\n\",\n",
    "                \"Now let's prepare the data for modeling and train a Random Forest classifier.\"\n",
    "            ]\n",
    "        },\n",
    "        {\n",
    "            \"cell_type\": \"code\",\n",
    "            \"execution_count\": 9,\n",
    "            \"metadata\": {},\n",
    "            \"source\": [\n",
    "                \"# Select features and target\\n\",\n",
    "                \"features = ['amount', 'is_foreign_receiver', 'num_recent_transactions', 'avg_transaction_amount',\\n\",\n",
    "                \"           'transaction_frequency_change', 'is_new_receiver', 'time_of_day_risk',\\n\",\n",
    "                \"           'amount_avg_ratio', 'combined_risk_score', 'amount_risk', 'history_risk', 'risk_score']\\n\",\n",
    "                \"\\n\",\n",
    "                \"X = enhanced_df[features]\\n\",\n",
    "                \"y = enhanced_df['is_fraud']\\n\",\n",
    "                \"\\n\",\n",
    "                \"# Split the data into training and testing sets\\n\",\n",
    "                \"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42, stratify=y)\\n\",\n",
    "                \"\\n\",\n",
    "                \"print(f\\\"Training set shape: {X_train.shape}\\\")\\n\",\n",
    "                \"print(f\\\"Testing set shape: {X_test.shape}\\\")\\n\",\n",
    "                \"print(f\\\"Fraud ratio in training set: {y_train.mean():.4f}\\\")\\n\",\n",
    "                \"print(f\\\"Fraud ratio in testing set: {y_test.mean():.4f}\\\")\"\n",
    "            ]\n",
    "        },\n",
    "        {\n",
    "            \"cell_type\": \"code\",\n",
    "            \"execution_count\": 10,\n",
    "            \"metadata\": {},\n",
    "            \"source\": [\n",
    "                \"# Apply SMOTE to handle class imbalance\\n\",\n",
    "                \"smote = SMOTE(random_state=42)\\n\",\n",
    "                \"X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\\n\",\n",
    "                \"\\n\",\n",
    "                \"print(f\\\"Original training set shape: {X_train.shape}\\\")\\n\",\n",
    "                \"print(f\\\"Resampled training set shape: {X_train_resampled.shape}\\\")\\n\",\n",
    "                \"print(f\\\"Original fraud ratio in training set: {y_train.mean():.4f}\\\")\\n\",\n",
    "                \"print(f\\\"Resampled fraud ratio in training set: {y_train_resampled.mean():.4f}\\\")\"\n",
    "            ]\n",
    "        },\n",
    "        {\n",
    "            \"cell_type\": \"code\",\n",
    "            \"execution_count\": 11,\n",
    "            \"metadata\": {},\n",
    "            \"source\": [\n",
    "                \"# Train a Random Forest classifier\\n\",\n",
    "                \"rf_model = RandomForestClassifier(random_state=42)\\n\",\n",
    "                \"\\n\",\n",
    "                \"# Define parameter grid for hyperparameter tuning\\n\",\n",
    "                \"param_grid = {\\n\",\n",
    "                \"    'n_estimators': [50, 100, 200],\\n\",\n",
    "                \"    'max_depth': [None, 10, 20, 30],\\n\",\n",
    "                \"    'min_samples_split': [2, 5, 10],\\n\",\n",
    "                \"    'min_samples_leaf': [1, 2, 4]\\n\",\n",
    "                \"}\\n\",\n",
    "                \"\\n\",\n",
    "                \"# Use GridSearchCV to find the best hyperparameters\\n\",\n",
    "                \"grid_search = GridSearchCV(rf_model, param_grid, cv=5, scoring='f1', n_jobs=-1)\\n\",\n",
    "                \"grid_search.fit(X_train_resampled, y_train_resampled)\\n\",\n",
    "                \"\\n\",\n",
    "                \"# Get the best model\\n\",\n",
    "                \"best_rf_model = grid_search.best_estimator_\\n\",\n",
    "                \"print(f\\\"Best hyperparameters: {grid_search.best_params_}\\\")\\n\",\n",
    "                \"print(f\\\"Best cross-validation F1 score: {grid_search.best_score_:.4f}\\\")\"\n",
    "            ]\n",
    "        },\n",
    "        {\n",
    "            \"cell_type\": \"code\",\n",
    "            \"execution_count\": 12,\n",
    "            \"metadata\": {},\n",
    "            \"source\": [\n",
    "                \"# Evaluate the model on the test set\\n\",\n",
    "                \"y_pred = best_rf_model.predict(X_test)\\n\",\n",
    "                \"y_prob = best_rf_model.predict_proba(X_test)[:, 1]\\n\",\n",
    "                \"\\n\",\n",
    "                \"# Classification report\\n\",\n",
    "                \"print(\\\"Classification Report:\\\")\\n\",\n",
    "                \"print(classification_report(y_test, y_pred))\\n\",\n",
    "                \"\\n\",\n",
    "                \"# Confusion matrix\\n\",\n",
    "                \"conf_matrix = confusion_matrix(y_test, y_pred)\\n\",\n",
    "                \"plt.figure(figsize=(8, 6))\\n\",\n",
    "                \"sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues',\\n\",\n",
    "                \"           xticklabels=['Legitimate', 'Fraudulent'],\\n\",\n",
    "                \"           yticklabels=['Legitimate', 'Fraudulent'])\\n\",\n",
    "                \"plt.title('Confusion Matrix')\\n\",\n",
    "                \"plt.xlabel('Predicted')\\n\",\n",
    "                \"plt.ylabel('Actual')\\n\",\n",
    "                \"plt.show()\"\n",
    "            ]\n",
    "        },\n",
    "        {\n",
    "            \"cell_type\": \"code\",\n",
    "            \"execution_count\": 13,\n",
    "            \"metadata\": {},\n",
    "            \"source\": [\n",
    "                \"# ROC curve\\n\",\n",
    "                \"fpr, tpr, _ = roc_curve(y_test, y_prob)\\n\",\n",
    "                \"roc_auc = auc(fpr, tpr)\\n\",\n",
    "                \"\\n\",\n",
    "                \"plt.figure(figsize=(8, 6))\\n\",\n",
    "                \"plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')\\n\",\n",
    "                \"plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\\n\",\n",
    "                \"plt.xlim([0.0, 1.0])\\n\",\n",
    "                \"plt.ylim([0.0, 1.05])\\n\",\n",
    "                \"plt.xlabel('False Positive Rate')\\n\",\n",
    "                \"plt.ylabel('True Positive Rate')\\n\",\n",
    "                \"plt.title('Receiver Operating Characteristic (ROC) Curve')\\n\",\n",
    "                \"plt.legend(loc='lower right')\\n\",\n",
    "                \"plt.show()\\n\",\n",
    "                \"\\n\",\n",
    "                \"# Feature importance\\n\",\n",
    "                \"feature_importance = pd.DataFrame({\\n\",\n",
    "                \"    'Feature': features,\\n\",\n",
    "                \"    'Importance': best_rf_model.feature_importances_\\n\",\n",
    "                \"}).sort_values(by='Importance', ascending=False)\\n\",\n",
    "                \"\\n\",\n",
    "                \"plt.figure(figsize=(10, 6))\\n\",\n",
    "                \"sns.barplot(data=feature_importance, x='Importance', y='Feature')\\n\",\n",
    "                \"plt.title('Feature Importance')\\n\",\n",
    "                \"plt.xlabel('Importance')\\n\",\n",
    "                \"plt.tight_layout()\\n\",\n",
    "                \"plt.show()\"\n",
    "            ]\n",
    "        },\n",
    "        {\n",
    "            \"cell_type\": \"markdown\",\n",
    "            \"metadata\": {},\n",
    "            \"source\": [\n",
    "                \"## 5. Model Tuning and Threshold Optimization\\n\",\n",
    "                \"\\n\",\n",
    "                \"For fraud detection, we often need to adjust the classification threshold to balance between false positives and false negatives.\"\n",
    "            ]\n",
    "        },\n",
    "        {\n",
    "            \"cell_type\": \"code\",\n",
    "            \"execution_count\": 14,\n",
    "            \"metadata\": {},\n",
    "            \"source\": [\n",
    "                \"# Calculate and plot precision-recall curve\\n\",\n",
    "                \"from sklearn.metrics import precision_recall_curve, average_precision_score\\n\",\n",
    "                \"\\n\",\n",
    "                \"precision, recall, thresholds = precision_recall_curve(y_test, y_prob)\\n\",\n",
    "                \"avg_precision = average_precision_score(y_test, y_prob)\\n\",\n",
    "                \"\\n\",\n",
    "                \"plt.figure(figsize=(10, 6))\\n\",\n",
    "                \"plt.plot(recall, precision, color='blue', lw=2, label=f'Precision-Recall curve (AP = {avg_precision:.2f})')\\n\",\n",
    "                \"plt.xlabel('Recall')\\n\",\n",
    "                \"plt.ylabel('Precision')\\n\",\n",
    "                \"plt.title('Precision-Recall Curve')\\n\",\n",
    "                \"plt.legend(loc='best')\\n\",\n",
    "                \"plt.grid(True)\\n\",\n",
    "                \"plt.show()\\n\",\n",
    "                \"\\n\",\n",
    "                \"# Plot thresholds vs precision and recall\\n\",\n",
    "                \"plt.figure(figsize=(10, 6))\\n\",\n",
    "                \"plt.plot(thresholds, precision[:-1], 'b--', label='Precision')\\n\",\n",
    "                \"plt.plot(thresholds, recall[:-1], 'g-', label='Recall')\\n\",\n",
    "                \"plt.xlabel('Threshold')\\n\",\n",
    "                \"plt.title('Precision and Recall vs. Threshold')\\n\",\n",
    "                \"plt.legend()\\n\",\n",
    "                \"plt.grid(True)\\n\",\n",
    "                \"plt.show()\"\n",
    "            ]\n",
    "        },\n",
    "        {\n",
    "            \"cell_type\": \"code\",\n",
    "            \"execution_count\": 15,\n",
    "            \"metadata\": {},\n",
    "            \"source\": [\n",
    "                \"# Calculate F1 score for different thresholds\\n\",\n",
    "                \"from sklearn.metrics import f1_score\\n\",\n",
    "                \"\\n\",\n",
    "                \"f1_scores = []\\n\",\n",
    "                \"thresholds_to_try = np.arange(0.1, 0.9, 0.05)\\n\",\n",
    "                \"\\n\",\n",
    "                \"for threshold in thresholds_to_try:\\n\",\n",
    "                \"    y_pred_threshold = (y_prob >= threshold).astype(int)\\n\",\n",
    "                \"    f1 = f1_score(y_test, y_pred_threshold)\\n\",\n",
    "                \"    f1_scores.append(f1)\\n\",\n",
    "                \"\\n\",\n",
    "                \"# Find the threshold with the highest F1 score\\n\",\n",
    "                \"best_threshold_idx = np.argmax(f1_scores)\\n\",\n",
    "                \"best_threshold = thresholds_to_try[best_threshold_idx]\\n\",\n",
    "                \"best_f1 = f1_scores[best_threshold_idx]\\n\",\n",
    "                \"\\n\",\n",
    "                \"plt.figure(figsize=(10, 6))\\n\",\n",
    "                \"plt.plot(thresholds_to_try, f1_scores, 'r-')\\n\",\n",
    "                \"plt.axvline(x=best_threshold, color='green', linestyle='--', \\n\",\n",
    "                \"           label=f'Best Threshold = {best_threshold:.2f}, F1 = {best_f1:.2f}')\\n\",\n",
    "                \"plt.xlabel('Threshold')\\n\",\n",
    "                \"plt.ylabel('F1 Score')\\n\",\n",
    "                \"plt.title('F1 Score vs. Threshold')\\n\",\n",
    "                \"plt.legend()\\n\",\n",
    "                \"plt.grid(True)\\n\",\n",
    "                \"plt.show()\\n\",\n",
    "                \"\\n\",\n",
    "                \"print(f\\\"Best threshold: {best_threshold:.2f}\\\")\\n\",\n",
    "                \"print(f\\\"F1 score at best threshold: {best_f1:.4f}\\\")\"\n",
    "            ]\n",
    "        },\n",
    "        {\n",
    "            \"cell_type\": \"code\",\n",
    "            \"execution_count\": 16,\n",
    "            \"metadata\": {},\n",
    "            \"source\": [\n",
    "                \"# Evaluate the model with the optimized threshold\\n\",\n",
    "                \"y_pred_optimized = (y_prob >= best_threshold).astype(int)\\n\",\n",
    "                \"\\n\",\n",
    "                \"print(\\\"Classification Report with Optimized Threshold:\\\")\\n\",\n",
    "                \"print(classification_report(y_test, y_pred_optimized))\\n\",\n",
    "                \"\\n\",\n",
    "                \"# Confusion matrix with optimized threshold\\n\",\n",
    "                \"conf_matrix_opt = confusion_matrix(y_test, y_pred_optimized)\\n\",\n",
    "                \"plt.figure(figsize=(8, 6))\\n\",\n",
    "                \"sns.heatmap(conf_matrix_opt, annot=True, fmt='d', cmap='Blues',\\n\",\n",
    "                \"           xticklabels=['Legitimate', 'Fraudulent'],\\n\",\n",
    "                \"           yticklabels=['Legitimate', 'Fraudulent'])\\n\",\n",
    "                \"plt.title('Confusion Matrix with Optimized Threshold')\\n\",\n",
    "                \"plt.xlabel('Predicted')\\n\",\n",
    "                \"plt.ylabel('Actual')\\n\",\n",
    "                \"plt.show()\"\n",
    "            ]\n",
    "        },\n",
    "        {\n",
    "            \"cell_type\": \"markdown\",\n",
    "            \"metadata\": {},\n",
    "            \"source\": [\n",
    "                \"## 6. Save the Model\\n\",\n",
    "                \"\\n\",\n",
    "                \"Let's save the trained model for later use in the MoMoGuard-GH application.\"\n",
    "            ]\n",
    "        },\n",
    "        {\n",
    "            \"cell_type\": \"code\",\n",
    "            \"execution_count\": 17,\n",
    "            \"metadata\": {},\n",
    "            \"source\": [\n",
    "                \"# Create a model package with the model, optimized threshold, and feature list\\n\",\n",
    "                \"model_package = {\\n\",\n",
    "                \"    'model': best_rf_model,\\n\",\n",
    "                \"    'threshold': best_threshold,\\n\",\n",
    "                \"    'features': features\\n\",\n",
    "                \"}\\n\",\n",
    "                \"\\n\",\n",
    "                \"# Save the model package\\n\",\n",
    "                \"joblib.dump(model_package, '../models/momoguard_gh_model.pkl')\\n\",\n",
    "                \"print(\\\"Model package saved successfully!\\\")\"\n",
    "            ]\n",
    "        },\n",
    "        {\n",
    "            \"cell_type\": \"markdown\",\n",
    "            \"metadata\": {},\n",
    "            \"source\": [\n",
    "                \"## 7. Example Real-time Fraud Detection Function\\n\",\n",
    "                \"\\n\",\n",
    "                \"Let's create a function that can be used in production to detect fraud in real-time.\"\n",
    "            ]\n",
    "        },\n",
    "        {\n",
    "            \"cell_type\": \"code\",\n",
    "            \"execution_count\": 18,\n",
    "            \"metadata\": {},\n",
    "            \"source\": [\n",
    "                \"def detect_fraud(transaction_data, model_package):\\n\",\n",
    "                \"    \\\"\\\"\\\"\\n\",\n",
    "                \"    Detect potential fraud in a mobile money transaction\\n\",\n",
    "                \"    \\n\",\n",
    "                \"    Parameters:\\n\",\n",
    "                \"    transaction_data (dict): Transaction data with required fields\\n\",\n",
    "                \"    model_package (dict): Model package containing the model, threshold, and features\\n\",\n",
    "                \"    \\n\",\n",
    "                \"    Returns:\\n\",\n",
    "                \"    dict: Fraud detection results\\n\",\n",
    "                \"    \\\"\\\"\\\"\\n\",\n",
    "                \"    # Extract model components\\n\",\n",
    "                \"    model = model_package['model']\\n\",\n",
    "                \"    threshold = model_package['threshold']\\n\",\n",
    "                \"    features = model_package['features']\\n\",\n",
    "                \"    \\n\",\n",
    "                \"    # Apply feature engineering to transaction data\\n\",\n",
    "                \"    # (In production, this would pull additional data like user history)\\n\",\n",
    "                \"    transaction_df = pd.DataFrame([transaction_data])\\n\",\n",
    "                \"    \\n\",\n",
    "                \"    #\"\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "}"
   ],
   "id": "5be1959de40509d4",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cells': [{'cell_type': 'markdown',\n",
       "   'metadata': {},\n",
       "   'source': ['# MoMoGuard-GH: Fraud Detection Model Training\\n',\n",
       "    '\\n',\n",
       "    \"This notebook demonstrates the process of training a machine learning model to detect fraudulent mobile money transactions. For the MVP, we'll use synthetic data that simulates typical fraud patterns in Ghana's mobile money ecosystem.\"]},\n",
       "  {'cell_type': 'code',\n",
       "   'execution_count': 1,\n",
       "   'metadata': {},\n",
       "   'source': ['import numpy as np\\n',\n",
       "    'import pandas as pd\\n',\n",
       "    'import matplotlib.pyplot as plt\\n',\n",
       "    'import seaborn as sns\\n',\n",
       "    'from sklearn.model_selection import train_test_split, GridSearchCV\\n',\n",
       "    'from sklearn.ensemble import RandomForestClassifier\\n',\n",
       "    'from sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc\\n',\n",
       "    'from imblearn.over_sampling import SMOTE\\n',\n",
       "    'import joblib\\n',\n",
       "    '\\n',\n",
       "    '# Set random seed for reproducibility\\n',\n",
       "    'np.random.seed(42)']},\n",
       "  {'cell_type': 'markdown',\n",
       "   'metadata': {},\n",
       "   'source': ['## 1. Generate Synthetic Mobile Money Transaction Data\\n',\n",
       "    '\\n',\n",
       "    \"Since real mobile money transaction data is sensitive and not easily available, we'll create synthetic data that resembles typical mobile money transactions in Ghana, with some fraudulent patterns included.\"]},\n",
       "  {'cell_type': 'code',\n",
       "   'execution_count': 2,\n",
       "   'metadata': {},\n",
       "   'source': ['def generate_synthetic_data(n_samples=10000, fraud_ratio=0.05):\\n',\n",
       "    '    \"\"\"\\n',\n",
       "    '    Generate synthetic mobile money transaction data with fraud indicators\\n',\n",
       "    '    \\n',\n",
       "    '    Parameters:\\n',\n",
       "    '    n_samples (int): Number of transactions to generate\\n',\n",
       "    '    fraud_ratio (float): Ratio of fraudulent transactions\\n',\n",
       "    '    \\n',\n",
       "    '    Returns:\\n',\n",
       "    '    DataFrame: Synthetic transaction data\\n',\n",
       "    '    \"\"\"\\n',\n",
       "    '    # Number of fraudulent and legitimate transactions\\n',\n",
       "    '    n_fraud = int(n_samples * fraud_ratio)\\n',\n",
       "    '    n_legitimate = n_samples - n_fraud\\n',\n",
       "    '    \\n',\n",
       "    '    # Generate legitimate transactions\\n',\n",
       "    '    legitimate_data = {\\n',\n",
       "    \"        'amount': np.random.gamma(2, 100, n_legitimate),  # Typical transaction amounts\\n\",\n",
       "    \"        'is_foreign_receiver': np.random.choice([0, 1], size=n_legitimate, p=[0.95, 0.05]),  # Most receivers are local\\n\",\n",
       "    \"        'num_recent_transactions': np.random.poisson(10, n_legitimate),  # Average transaction history\\n\",\n",
       "    \"        'avg_transaction_amount': np.random.gamma(2, 80, n_legitimate),  # Typical average amounts\\n\",\n",
       "    \"        'transaction_frequency_change': np.random.normal(0, 0.3, n_legitimate),  # Stable transaction patterns\\n\",\n",
       "    \"        'is_new_receiver': np.random.choice([0, 1], size=n_legitimate, p=[0.7, 0.3]),  # Most receivers are known\\n\",\n",
       "    \"        'time_of_day_risk': np.random.choice([0, 1], size=n_legitimate, p=[0.85, 0.15]),  # Mostly daytime transactions\\n\",\n",
       "    \"        'is_fraud': np.zeros(n_legitimate)  # Not fraud\\n\",\n",
       "    '    }\\n',\n",
       "    '    \\n',\n",
       "    '    # Generate fraudulent transactions with distinct patterns\\n',\n",
       "    '    fraud_data = {\\n',\n",
       "    \"        'amount': np.random.gamma(5, 150, n_fraud),  # Higher amounts on average\\n\",\n",
       "    \"        'is_foreign_receiver': np.random.choice([0, 1], size=n_fraud, p=[0.6, 0.4]),  # More foreign receivers\\n\",\n",
       "    \"        'num_recent_transactions': np.random.poisson(3, n_fraud),  # Less transaction history\\n\",\n",
       "    \"        'avg_transaction_amount': np.random.gamma(1.5, 50, n_fraud),  # Lower historical amounts\\n\",\n",
       "    \"        'transaction_frequency_change': np.random.normal(1.5, 0.8, n_fraud),  # Sudden increase in frequency\\n\",\n",
       "    \"        'is_new_receiver': np.random.choice([0, 1], size=n_fraud, p=[0.2, 0.8]),  # Mostly new receivers\\n\",\n",
       "    \"        'time_of_day_risk': np.random.choice([0, 1], size=n_fraud, p=[0.3, 0.7]),  # More night transactions\\n\",\n",
       "    \"        'is_fraud': np.ones(n_fraud)  # Is fraud\\n\",\n",
       "    '    }\\n',\n",
       "    '    \\n',\n",
       "    '    # Combine legitimate and fraudulent data\\n',\n",
       "    '    for key in legitimate_data:\\n',\n",
       "    '        legitimate_data[key] = np.concatenate([legitimate_data[key], fraud_data[key]])\\n',\n",
       "    '        \\n',\n",
       "    '    # Create DataFrame\\n',\n",
       "    '    df = pd.DataFrame(legitimate_data)\\n',\n",
       "    '    \\n',\n",
       "    '    # Shuffle the data\\n',\n",
       "    '    df = df.sample(frac=1).reset_index(drop=True)\\n',\n",
       "    '    \\n',\n",
       "    '    return df\\n',\n",
       "    '\\n',\n",
       "    '# Generate data\\n',\n",
       "    'transactions_df = generate_synthetic_data()\\n',\n",
       "    '\\n',\n",
       "    '# Display first few rows\\n',\n",
       "    'transactions_df.head()']},\n",
       "  {'cell_type': 'code',\n",
       "   'execution_count': 3,\n",
       "   'metadata': {},\n",
       "   'source': ['# Save synthetic data to CSV\\n',\n",
       "    \"transactions_df.to_csv('../data/sim_swap_fraud.csv', index=False)\\n\",\n",
       "    '\\n',\n",
       "    '# Basic statistics\\n',\n",
       "    'print(f\"Total transactions: {len(transactions_df)}\")\\n',\n",
       "    'print(f\"Fraudulent transactions: {transactions_df[\\'is_fraud\\'].sum()} ({transactions_df[\\'is_fraud\\'].mean()*100:.2f}%)\")']},\n",
       "  {'cell_type': 'markdown',\n",
       "   'metadata': {},\n",
       "   'source': ['## 2. Exploratory Data Analysis']},\n",
       "  {'cell_type': 'code',\n",
       "   'execution_count': 4,\n",
       "   'metadata': {},\n",
       "   'source': ['# Summary statistics\\n',\n",
       "    'print(\"Summary statistics for all transactions:\")\\n',\n",
       "    'transactions_df.describe()\\n',\n",
       "    '\\n',\n",
       "    '# Summary statistics by fraud status\\n',\n",
       "    'print(\"\\\\nSummary statistics for legitimate transactions:\")\\n',\n",
       "    \"transactions_df[transactions_df['is_fraud'] == 0].describe()\\n\",\n",
       "    '\\n',\n",
       "    'print(\"\\\\nSummary statistics for fraudulent transactions:\")\\n',\n",
       "    \"transactions_df[transactions_df['is_fraud'] == 1].describe()\"]},\n",
       "  {'cell_type': 'code',\n",
       "   'execution_count': 5,\n",
       "   'metadata': {},\n",
       "   'source': ['# Set up the visualization style\\n',\n",
       "    \"plt.style.use('seaborn-whitegrid')\\n\",\n",
       "    \"plt.rcParams['figure.figsize'] = (12, 8)\\n\",\n",
       "    \"plt.rcParams['font.size'] = 12\\n\",\n",
       "    '\\n',\n",
       "    '# Distribution of transaction amounts by fraud status\\n',\n",
       "    'plt.figure(figsize=(12, 6))\\n',\n",
       "    \"sns.histplot(data=transactions_df, x='amount', hue='is_fraud', bins=50, kde=True, element='step')\\n\",\n",
       "    \"plt.title('Distribution of Transaction Amounts by Fraud Status')\\n\",\n",
       "    \"plt.xlabel('Transaction Amount (GHS)')\\n\",\n",
       "    \"plt.ylabel('Count')\\n\",\n",
       "    \"plt.legend(['Legitimate', 'Fraudulent'])\\n\",\n",
       "    'plt.xlim(0, 1500)  # Limit x-axis for better visualization\\n',\n",
       "    'plt.show()\\n',\n",
       "    '\\n',\n",
       "    '# Transaction frequency change by fraud status\\n',\n",
       "    'plt.figure(figsize=(12, 6))\\n',\n",
       "    \"sns.boxplot(data=transactions_df, x='is_fraud', y='transaction_frequency_change')\\n\",\n",
       "    \"plt.title('Transaction Frequency Change by Fraud Status')\\n\",\n",
       "    \"plt.xlabel('Is Fraud')\\n\",\n",
       "    \"plt.ylabel('Transaction Frequency Change')\\n\",\n",
       "    \"plt.xticks([0, 1], ['Legitimate', 'Fraudulent'])\\n\",\n",
       "    'plt.show()']},\n",
       "  {'cell_type': 'code',\n",
       "   'execution_count': 6,\n",
       "   'metadata': {},\n",
       "   'source': ['# Correlation matrix\\n',\n",
       "    'plt.figure(figsize=(10, 8))\\n',\n",
       "    'correlation_matrix = transactions_df.corr()\\n',\n",
       "    \"sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', vmin=-1, vmax=1, fmt='.2f')\\n\",\n",
       "    \"plt.title('Correlation Matrix of Transaction Features')\\n\",\n",
       "    'plt.show()\\n',\n",
       "    '\\n',\n",
       "    '# Feature distribution comparisons\\n',\n",
       "    'fig, axes = plt.subplots(2, 3, figsize=(18, 10))\\n',\n",
       "    'axes = axes.flatten()\\n',\n",
       "    '\\n',\n",
       "    '# Plot histograms for numerical features\\n',\n",
       "    \"numerical_features = ['amount', 'num_recent_transactions', 'avg_transaction_amount', \\n\",\n",
       "    \"                     'transaction_frequency_change']\\n\",\n",
       "    '\\n',\n",
       "    'for i, feature in enumerate(numerical_features):\\n',\n",
       "    \"    sns.histplot(data=transactions_df, x=feature, hue='is_fraud', kde=True, element='step', ax=axes[i])\\n\",\n",
       "    '    axes[i].set_title(f\\'Distribution of {feature.replace(\"_\", \" \").title()}\\')\\n',\n",
       "    \"    axes[i].legend(['Legitimate', 'Fraudulent'])\\n\",\n",
       "    '\\n',\n",
       "    '# Plot count plots for categorical features\\n',\n",
       "    \"categorical_features = ['is_foreign_receiver', 'is_new_receiver', 'time_of_day_risk']\\n\",\n",
       "    'for i, feature in enumerate(categorical_features):\\n',\n",
       "    \"    sns.countplot(data=transactions_df, x=feature, hue='is_fraud', ax=axes[i+len(numerical_features)])\\n\",\n",
       "    '    axes[i+len(numerical_features)].set_title(f\\'Count of {feature.replace(\"_\", \" \").title()}\\')\\n',\n",
       "    \"    axes[i+len(numerical_features)].legend(['Legitimate', 'Fraudulent'])\\n\",\n",
       "    '\\n',\n",
       "    'plt.tight_layout()\\n',\n",
       "    'plt.show()']},\n",
       "  {'cell_type': 'markdown',\n",
       "   'metadata': {},\n",
       "   'source': ['## 3. Feature Engineering\\n',\n",
       "    '\\n',\n",
       "    \"Let's create some additional features that might be helpful for fraud detection.\"]},\n",
       "  {'cell_type': 'code',\n",
       "   'execution_count': 7,\n",
       "   'metadata': {},\n",
       "   'source': ['def engineer_features(df):\\n',\n",
       "    '    \"\"\"\\n',\n",
       "    '    Create new features for fraud detection\\n',\n",
       "    '    \\n',\n",
       "    '    Parameters:\\n',\n",
       "    '    df (DataFrame): Original transaction data\\n',\n",
       "    '    \\n',\n",
       "    '    Returns:\\n',\n",
       "    '    DataFrame: Data with additional features\\n',\n",
       "    '    \"\"\"\\n',\n",
       "    '    # Create a copy to avoid modifying the original\\n',\n",
       "    '    df_new = df.copy()\\n',\n",
       "    '    \\n',\n",
       "    '    # Amount ratio compared to average (unusual amounts)\\n',\n",
       "    \"    df_new['amount_avg_ratio'] = df_new['amount'] / (df_new['avg_transaction_amount'] + 1)  # +1 to avoid division by zero\\n\",\n",
       "    '    \\n',\n",
       "    '    # Combine risk factors\\n',\n",
       "    \"    df_new['combined_risk_score'] = (\\n\",\n",
       "    \"        df_new['is_foreign_receiver'] * 2 +\\n\",\n",
       "    \"        df_new['is_new_receiver'] * 1.5 +\\n\",\n",
       "    \"        df_new['time_of_day_risk'] * 1 +\\n\",\n",
       "    \"        (df_new['transaction_frequency_change'] > 1) * 2\\n\",\n",
       "    '    )\\n',\n",
       "    '    \\n',\n",
       "    '    # Transaction amount risk (high amounts are riskier)\\n',\n",
       "    \"    df_new['amount_risk'] = np.log1p(df_new['amount']) / 10  # Log-transform and scale\\n\",\n",
       "    '    \\n',\n",
       "    '    # Transaction history risk (fewer transactions means higher risk)\\n',\n",
       "    \"    df_new['history_risk'] = np.exp(-df_new['num_recent_transactions'] / 10)\\n\",\n",
       "    '    \\n',\n",
       "    '    # Compute overall risk score\\n',\n",
       "    \"    df_new['risk_score'] = (\\n\",\n",
       "    \"        df_new['combined_risk_score'] * 0.4 +\\n\",\n",
       "    \"        df_new['amount_risk'] * 0.3 +\\n\",\n",
       "    \"        df_new['history_risk'] * 0.3\\n\",\n",
       "    '    )\\n',\n",
       "    '    \\n',\n",
       "    '    return df_new\\n',\n",
       "    '\\n',\n",
       "    '# Apply feature engineering\\n',\n",
       "    'enhanced_df = engineer_features(transactions_df)\\n',\n",
       "    '\\n',\n",
       "    '# Display first few rows with new features\\n',\n",
       "    'enhanced_df.head()']},\n",
       "  {'cell_type': 'code',\n",
       "   'execution_count': 8,\n",
       "   'metadata': {},\n",
       "   'source': ['# Analyze new features\\n',\n",
       "    'plt.figure(figsize=(12, 6))\\n',\n",
       "    \"sns.boxplot(data=enhanced_df, x='is_fraud', y='risk_score')\\n\",\n",
       "    \"plt.title('Risk Score by Fraud Status')\\n\",\n",
       "    \"plt.xlabel('Is Fraud')\\n\",\n",
       "    \"plt.ylabel('Risk Score')\\n\",\n",
       "    \"plt.xticks([0, 1], ['Legitimate', 'Fraudulent'])\\n\",\n",
       "    'plt.show()\\n',\n",
       "    '\\n',\n",
       "    '# Correlation of new features with fraud\\n',\n",
       "    \"new_features_corr = enhanced_df[['amount_avg_ratio', 'combined_risk_score', 'amount_risk', \\n\",\n",
       "    \"                               'history_risk', 'risk_score', 'is_fraud']].corr()['is_fraud'].sort_values()\\n\",\n",
       "    'plt.figure(figsize=(10, 6))\\n',\n",
       "    \"new_features_corr.drop('is_fraud').plot(kind='barh')\\n\",\n",
       "    \"plt.title('Correlation of New Features with Fraud')\\n\",\n",
       "    \"plt.xlabel('Correlation Coefficient')\\n\",\n",
       "    'plt.tight_layout()\\n',\n",
       "    'plt.show()']},\n",
       "  {'cell_type': 'markdown',\n",
       "   'metadata': {},\n",
       "   'source': ['## 4. Model Training\\n',\n",
       "    '\\n',\n",
       "    \"Now let's prepare the data for modeling and train a Random Forest classifier.\"]},\n",
       "  {'cell_type': 'code',\n",
       "   'execution_count': 9,\n",
       "   'metadata': {},\n",
       "   'source': ['# Select features and target\\n',\n",
       "    \"features = ['amount', 'is_foreign_receiver', 'num_recent_transactions', 'avg_transaction_amount',\\n\",\n",
       "    \"           'transaction_frequency_change', 'is_new_receiver', 'time_of_day_risk',\\n\",\n",
       "    \"           'amount_avg_ratio', 'combined_risk_score', 'amount_risk', 'history_risk', 'risk_score']\\n\",\n",
       "    '\\n',\n",
       "    'X = enhanced_df[features]\\n',\n",
       "    \"y = enhanced_df['is_fraud']\\n\",\n",
       "    '\\n',\n",
       "    '# Split the data into training and testing sets\\n',\n",
       "    'X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42, stratify=y)\\n',\n",
       "    '\\n',\n",
       "    'print(f\"Training set shape: {X_train.shape}\")\\n',\n",
       "    'print(f\"Testing set shape: {X_test.shape}\")\\n',\n",
       "    'print(f\"Fraud ratio in training set: {y_train.mean():.4f}\")\\n',\n",
       "    'print(f\"Fraud ratio in testing set: {y_test.mean():.4f}\")']},\n",
       "  {'cell_type': 'code',\n",
       "   'execution_count': 10,\n",
       "   'metadata': {},\n",
       "   'source': ['# Apply SMOTE to handle class imbalance\\n',\n",
       "    'smote = SMOTE(random_state=42)\\n',\n",
       "    'X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\\n',\n",
       "    '\\n',\n",
       "    'print(f\"Original training set shape: {X_train.shape}\")\\n',\n",
       "    'print(f\"Resampled training set shape: {X_train_resampled.shape}\")\\n',\n",
       "    'print(f\"Original fraud ratio in training set: {y_train.mean():.4f}\")\\n',\n",
       "    'print(f\"Resampled fraud ratio in training set: {y_train_resampled.mean():.4f}\")']},\n",
       "  {'cell_type': 'code',\n",
       "   'execution_count': 11,\n",
       "   'metadata': {},\n",
       "   'source': ['# Train a Random Forest classifier\\n',\n",
       "    'rf_model = RandomForestClassifier(random_state=42)\\n',\n",
       "    '\\n',\n",
       "    '# Define parameter grid for hyperparameter tuning\\n',\n",
       "    'param_grid = {\\n',\n",
       "    \"    'n_estimators': [50, 100, 200],\\n\",\n",
       "    \"    'max_depth': [None, 10, 20, 30],\\n\",\n",
       "    \"    'min_samples_split': [2, 5, 10],\\n\",\n",
       "    \"    'min_samples_leaf': [1, 2, 4]\\n\",\n",
       "    '}\\n',\n",
       "    '\\n',\n",
       "    '# Use GridSearchCV to find the best hyperparameters\\n',\n",
       "    \"grid_search = GridSearchCV(rf_model, param_grid, cv=5, scoring='f1', n_jobs=-1)\\n\",\n",
       "    'grid_search.fit(X_train_resampled, y_train_resampled)\\n',\n",
       "    '\\n',\n",
       "    '# Get the best model\\n',\n",
       "    'best_rf_model = grid_search.best_estimator_\\n',\n",
       "    'print(f\"Best hyperparameters: {grid_search.best_params_}\")\\n',\n",
       "    'print(f\"Best cross-validation F1 score: {grid_search.best_score_:.4f}\")']},\n",
       "  {'cell_type': 'code',\n",
       "   'execution_count': 12,\n",
       "   'metadata': {},\n",
       "   'source': ['# Evaluate the model on the test set\\n',\n",
       "    'y_pred = best_rf_model.predict(X_test)\\n',\n",
       "    'y_prob = best_rf_model.predict_proba(X_test)[:, 1]\\n',\n",
       "    '\\n',\n",
       "    '# Classification report\\n',\n",
       "    'print(\"Classification Report:\")\\n',\n",
       "    'print(classification_report(y_test, y_pred))\\n',\n",
       "    '\\n',\n",
       "    '# Confusion matrix\\n',\n",
       "    'conf_matrix = confusion_matrix(y_test, y_pred)\\n',\n",
       "    'plt.figure(figsize=(8, 6))\\n',\n",
       "    \"sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues',\\n\",\n",
       "    \"           xticklabels=['Legitimate', 'Fraudulent'],\\n\",\n",
       "    \"           yticklabels=['Legitimate', 'Fraudulent'])\\n\",\n",
       "    \"plt.title('Confusion Matrix')\\n\",\n",
       "    \"plt.xlabel('Predicted')\\n\",\n",
       "    \"plt.ylabel('Actual')\\n\",\n",
       "    'plt.show()']},\n",
       "  {'cell_type': 'code',\n",
       "   'execution_count': 13,\n",
       "   'metadata': {},\n",
       "   'source': ['# ROC curve\\n',\n",
       "    'fpr, tpr, _ = roc_curve(y_test, y_prob)\\n',\n",
       "    'roc_auc = auc(fpr, tpr)\\n',\n",
       "    '\\n',\n",
       "    'plt.figure(figsize=(8, 6))\\n',\n",
       "    \"plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')\\n\",\n",
       "    \"plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\\n\",\n",
       "    'plt.xlim([0.0, 1.0])\\n',\n",
       "    'plt.ylim([0.0, 1.05])\\n',\n",
       "    \"plt.xlabel('False Positive Rate')\\n\",\n",
       "    \"plt.ylabel('True Positive Rate')\\n\",\n",
       "    \"plt.title('Receiver Operating Characteristic (ROC) Curve')\\n\",\n",
       "    \"plt.legend(loc='lower right')\\n\",\n",
       "    'plt.show()\\n',\n",
       "    '\\n',\n",
       "    '# Feature importance\\n',\n",
       "    'feature_importance = pd.DataFrame({\\n',\n",
       "    \"    'Feature': features,\\n\",\n",
       "    \"    'Importance': best_rf_model.feature_importances_\\n\",\n",
       "    \"}).sort_values(by='Importance', ascending=False)\\n\",\n",
       "    '\\n',\n",
       "    'plt.figure(figsize=(10, 6))\\n',\n",
       "    \"sns.barplot(data=feature_importance, x='Importance', y='Feature')\\n\",\n",
       "    \"plt.title('Feature Importance')\\n\",\n",
       "    \"plt.xlabel('Importance')\\n\",\n",
       "    'plt.tight_layout()\\n',\n",
       "    'plt.show()']},\n",
       "  {'cell_type': 'markdown',\n",
       "   'metadata': {},\n",
       "   'source': ['## 5. Model Tuning and Threshold Optimization\\n',\n",
       "    '\\n',\n",
       "    'For fraud detection, we often need to adjust the classification threshold to balance between false positives and false negatives.']},\n",
       "  {'cell_type': 'code',\n",
       "   'execution_count': 14,\n",
       "   'metadata': {},\n",
       "   'source': ['# Calculate and plot precision-recall curve\\n',\n",
       "    'from sklearn.metrics import precision_recall_curve, average_precision_score\\n',\n",
       "    '\\n',\n",
       "    'precision, recall, thresholds = precision_recall_curve(y_test, y_prob)\\n',\n",
       "    'avg_precision = average_precision_score(y_test, y_prob)\\n',\n",
       "    '\\n',\n",
       "    'plt.figure(figsize=(10, 6))\\n',\n",
       "    \"plt.plot(recall, precision, color='blue', lw=2, label=f'Precision-Recall curve (AP = {avg_precision:.2f})')\\n\",\n",
       "    \"plt.xlabel('Recall')\\n\",\n",
       "    \"plt.ylabel('Precision')\\n\",\n",
       "    \"plt.title('Precision-Recall Curve')\\n\",\n",
       "    \"plt.legend(loc='best')\\n\",\n",
       "    'plt.grid(True)\\n',\n",
       "    'plt.show()\\n',\n",
       "    '\\n',\n",
       "    '# Plot thresholds vs precision and recall\\n',\n",
       "    'plt.figure(figsize=(10, 6))\\n',\n",
       "    \"plt.plot(thresholds, precision[:-1], 'b--', label='Precision')\\n\",\n",
       "    \"plt.plot(thresholds, recall[:-1], 'g-', label='Recall')\\n\",\n",
       "    \"plt.xlabel('Threshold')\\n\",\n",
       "    \"plt.title('Precision and Recall vs. Threshold')\\n\",\n",
       "    'plt.legend()\\n',\n",
       "    'plt.grid(True)\\n',\n",
       "    'plt.show()']},\n",
       "  {'cell_type': 'code',\n",
       "   'execution_count': 15,\n",
       "   'metadata': {},\n",
       "   'source': ['# Calculate F1 score for different thresholds\\n',\n",
       "    'from sklearn.metrics import f1_score\\n',\n",
       "    '\\n',\n",
       "    'f1_scores = []\\n',\n",
       "    'thresholds_to_try = np.arange(0.1, 0.9, 0.05)\\n',\n",
       "    '\\n',\n",
       "    'for threshold in thresholds_to_try:\\n',\n",
       "    '    y_pred_threshold = (y_prob >= threshold).astype(int)\\n',\n",
       "    '    f1 = f1_score(y_test, y_pred_threshold)\\n',\n",
       "    '    f1_scores.append(f1)\\n',\n",
       "    '\\n',\n",
       "    '# Find the threshold with the highest F1 score\\n',\n",
       "    'best_threshold_idx = np.argmax(f1_scores)\\n',\n",
       "    'best_threshold = thresholds_to_try[best_threshold_idx]\\n',\n",
       "    'best_f1 = f1_scores[best_threshold_idx]\\n',\n",
       "    '\\n',\n",
       "    'plt.figure(figsize=(10, 6))\\n',\n",
       "    \"plt.plot(thresholds_to_try, f1_scores, 'r-')\\n\",\n",
       "    \"plt.axvline(x=best_threshold, color='green', linestyle='--', \\n\",\n",
       "    \"           label=f'Best Threshold = {best_threshold:.2f}, F1 = {best_f1:.2f}')\\n\",\n",
       "    \"plt.xlabel('Threshold')\\n\",\n",
       "    \"plt.ylabel('F1 Score')\\n\",\n",
       "    \"plt.title('F1 Score vs. Threshold')\\n\",\n",
       "    'plt.legend()\\n',\n",
       "    'plt.grid(True)\\n',\n",
       "    'plt.show()\\n',\n",
       "    '\\n',\n",
       "    'print(f\"Best threshold: {best_threshold:.2f}\")\\n',\n",
       "    'print(f\"F1 score at best threshold: {best_f1:.4f}\")']},\n",
       "  {'cell_type': 'code',\n",
       "   'execution_count': 16,\n",
       "   'metadata': {},\n",
       "   'source': ['# Evaluate the model with the optimized threshold\\n',\n",
       "    'y_pred_optimized = (y_prob >= best_threshold).astype(int)\\n',\n",
       "    '\\n',\n",
       "    'print(\"Classification Report with Optimized Threshold:\")\\n',\n",
       "    'print(classification_report(y_test, y_pred_optimized))\\n',\n",
       "    '\\n',\n",
       "    '# Confusion matrix with optimized threshold\\n',\n",
       "    'conf_matrix_opt = confusion_matrix(y_test, y_pred_optimized)\\n',\n",
       "    'plt.figure(figsize=(8, 6))\\n',\n",
       "    \"sns.heatmap(conf_matrix_opt, annot=True, fmt='d', cmap='Blues',\\n\",\n",
       "    \"           xticklabels=['Legitimate', 'Fraudulent'],\\n\",\n",
       "    \"           yticklabels=['Legitimate', 'Fraudulent'])\\n\",\n",
       "    \"plt.title('Confusion Matrix with Optimized Threshold')\\n\",\n",
       "    \"plt.xlabel('Predicted')\\n\",\n",
       "    \"plt.ylabel('Actual')\\n\",\n",
       "    'plt.show()']},\n",
       "  {'cell_type': 'markdown',\n",
       "   'metadata': {},\n",
       "   'source': ['## 6. Save the Model\\n',\n",
       "    '\\n',\n",
       "    \"Let's save the trained model for later use in the MoMoGuard-GH application.\"]},\n",
       "  {'cell_type': 'code',\n",
       "   'execution_count': 17,\n",
       "   'metadata': {},\n",
       "   'source': ['# Create a model package with the model, optimized threshold, and feature list\\n',\n",
       "    'model_package = {\\n',\n",
       "    \"    'model': best_rf_model,\\n\",\n",
       "    \"    'threshold': best_threshold,\\n\",\n",
       "    \"    'features': features\\n\",\n",
       "    '}\\n',\n",
       "    '\\n',\n",
       "    '# Save the model package\\n',\n",
       "    \"joblib.dump(model_package, '../models/momoguard_gh_model.pkl')\\n\",\n",
       "    'print(\"Model package saved successfully!\")']},\n",
       "  {'cell_type': 'markdown',\n",
       "   'metadata': {},\n",
       "   'source': ['## 7. Example Real-time Fraud Detection Function\\n',\n",
       "    '\\n',\n",
       "    \"Let's create a function that can be used in production to detect fraud in real-time.\"]},\n",
       "  {'cell_type': 'code',\n",
       "   'execution_count': 18,\n",
       "   'metadata': {},\n",
       "   'source': ['def detect_fraud(transaction_data, model_package):\\n',\n",
       "    '    \"\"\"\\n',\n",
       "    '    Detect potential fraud in a mobile money transaction\\n',\n",
       "    '    \\n',\n",
       "    '    Parameters:\\n',\n",
       "    '    transaction_data (dict): Transaction data with required fields\\n',\n",
       "    '    model_package (dict): Model package containing the model, threshold, and features\\n',\n",
       "    '    \\n',\n",
       "    '    Returns:\\n',\n",
       "    '    dict: Fraud detection results\\n',\n",
       "    '    \"\"\"\\n',\n",
       "    '    # Extract model components\\n',\n",
       "    \"    model = model_package['model']\\n\",\n",
       "    \"    threshold = model_package['threshold']\\n\",\n",
       "    \"    features = model_package['features']\\n\",\n",
       "    '    \\n',\n",
       "    '    # Apply feature engineering to transaction data\\n',\n",
       "    '    # (In production, this would pull additional data like user history)\\n',\n",
       "    '    transaction_df = pd.DataFrame([transaction_data])\\n',\n",
       "    '    \\n',\n",
       "    '    #']}]}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 1
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
